#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

Grid Searchë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
Validation setìœ¼ë¡œ í‰ê°€í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.

í‰ê°€ ì§€í‘œ:
- Precision@K: ì¶”ì²œí•œ ì•„ì´í…œ ì¤‘ ì‹¤ì œë¡œ ì¢‹ì•„í•œ ë¹„ìœ¨
- Recall@K: ì¢‹ì•„í•  ì•„ì´í…œ ì¤‘ ì¶”ì²œí•œ ë¹„ìœ¨
- NDCG@K: ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì¶”ì²œ í’ˆì§ˆ
- Hit Rate@K: ìµœì†Œ 1ê°œë¼ë„ ë§ì¶˜ ì‚¬ìš©ì ë¹„ìœ¨

ì‚¬ìš©ë²•:
python hyperparameter_tuning.py
"""

import os
import sys
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
from itertools import product
import json
from datetime import datetime
from hybrid_recommender import HybridRecommender

class RecommendationEvaluator:
    """ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ í´ë˜ìŠ¤"""
    
    @staticmethod
    def precision_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """Precision@K ê³„ì‚°"""
        if k == 0:
            return 0.0
        
        recommended_k = recommended_items[:k]
        relevant_set = set(relevant_items)
        
        hits = sum(1 for item in recommended_k if item in relevant_set)
        return hits / k
    
    @staticmethod
    def recall_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """Recall@K ê³„ì‚°"""
        if len(relevant_items) == 0:
            return 0.0
        
        recommended_k = recommended_items[:k]
        relevant_set = set(relevant_items)
        
        hits = sum(1 for item in recommended_k if item in relevant_set)
        return hits / len(relevant_items)
    
    @staticmethod
    def dcg_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """DCG@K ê³„ì‚°"""
        recommended_k = recommended_items[:k]
        relevant_set = set(relevant_items)
        
        dcg = 0.0
        for i, item in enumerate(recommended_k):
            if item in relevant_set:
                # ê´€ë ¨ë„ëŠ” 1 (binary relevance)
                dcg += 1.0 / np.log2(i + 2)  # i+2 because index starts at 0
        
        return dcg
    
    @staticmethod
    def ndcg_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """NDCG@K ê³„ì‚°"""
        dcg = RecommendationEvaluator.dcg_at_k(recommended_items, relevant_items, k)
        
        # Ideal DCG (ëª¨ë“  ê´€ë ¨ ì•„ì´í…œì´ ìƒìœ„ì— ìˆì„ ë•Œ)
        ideal_relevant = relevant_items[:k]
        idcg = RecommendationEvaluator.dcg_at_k(ideal_relevant, relevant_items, k)
        
        if idcg == 0.0:
            return 0.0
        
        return dcg / idcg
    
    @staticmethod
    def hit_rate_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """Hit Rate@K ê³„ì‚° (ìµœì†Œ 1ê°œë¼ë„ ë§ì¶”ë©´ 1, ì•„ë‹ˆë©´ 0)"""
        recommended_k = recommended_items[:k]
        relevant_set = set(relevant_items)
        
        hits = sum(1 for item in recommended_k if item in relevant_set)
        return 1.0 if hits > 0 else 0.0
    
    @classmethod
    def evaluate_recommendations(cls, recommender: HybridRecommender, 
                                val_data: pd.DataFrame, 
                                top_k: int = 250,
                                top_n: int = 50,
                                eval_k_list: List[int] = [5, 10, 20]) -> Dict:
        """Validation setì— ëŒ€í•œ ì¶”ì²œ í‰ê°€"""
        
        print(f"\nValidation set í‰ê°€ ì‹œì‘ (top_k={top_k}, top_n={top_n})...")
        
        # ì‚¬ìš©ìë³„ ê´€ë ¨ ì•„ì´í…œ (validation setì—ì„œ ìƒí˜¸ì‘ìš©í•œ ì•„ì´í…œ)
        user_relevant_items = val_data.groupby('user_idx')['item_idx'].apply(list).to_dict()
        
        results = {k: {
            'precision': [],
            'recall': [],
            'ndcg': [],
            'hit_rate': []
        } for k in eval_k_list}
        
        total_users = len(user_relevant_items)
        evaluated_users = 0
        
        for i, (user_id, relevant_items) in enumerate(user_relevant_items.items()):
            if i % 10 == 0:
                print(f"  ì§„í–‰ë¥ : {i}/{total_users} ì‚¬ìš©ì í‰ê°€ ì™„ë£Œ...", end='\r')
            
            try:
                # ì¶”ì²œ ìƒì„±
                recommendations = recommender.get_recommendations(
                    user_id=str(user_id),
                    top_k=top_k,
                    top_n=top_n
                )
                
                if not recommendations:
                    continue
                
                # ì¶”ì²œëœ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
                recommended_items = [rec['item_id'] for rec in recommendations]
                
                # ê° Kì— ëŒ€í•´ í‰ê°€
                for k in eval_k_list:
                    results[k]['precision'].append(
                        cls.precision_at_k(recommended_items, relevant_items, k)
                    )
                    results[k]['recall'].append(
                        cls.recall_at_k(recommended_items, relevant_items, k)
                    )
                    results[k]['ndcg'].append(
                        cls.ndcg_at_k(recommended_items, relevant_items, k)
                    )
                    results[k]['hit_rate'].append(
                        cls.hit_rate_at_k(recommended_items, relevant_items, k)
                    )
                
                evaluated_users += 1
                
            except Exception as e:
                # ì¶”ì²œ ì‹¤íŒ¨í•œ ì‚¬ìš©ìëŠ” ìŠ¤í‚µ
                continue
        
        print(f"\n  í‰ê°€ ì™„ë£Œ: {evaluated_users}/{total_users} ì‚¬ìš©ì")
        
        # í‰ê·  ê³„ì‚°
        metrics = {}
        for k in eval_k_list:
            metrics[f'Precision@{k}'] = np.mean(results[k]['precision']) if results[k]['precision'] else 0.0
            metrics[f'Recall@{k}'] = np.mean(results[k]['recall']) if results[k]['recall'] else 0.0
            metrics[f'NDCG@{k}'] = np.mean(results[k]['ndcg']) if results[k]['ndcg'] else 0.0
            metrics[f'HitRate@{k}'] = np.mean(results[k]['hit_rate']) if results[k]['hit_rate'] else 0.0
        
        metrics['evaluated_users'] = evaluated_users
        metrics['total_users'] = total_users
        
        return metrics


def grid_search_hyperparameters(data_path: str, 
                                model_save_path: str,
                                param_grid: Dict,
                                eval_k_list: List[int] = [5, 10, 20],
                                device: str = 'cpu'):
    """Grid Searchë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°"""
    
    print("=" * 80)
    print("í•˜ì´í¼íŒŒë¼ë¯¸í„° Grid Search ì‹œì‘")
    print("=" * 80)
    
    # Validation ë°ì´í„° ë¡œë“œ
    val_path = os.path.join(data_path, "user_item_val.csv")
    if not os.path.exists(val_path):
        raise FileNotFoundError(f"Validation ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {val_path}")
    
    val_data = pd.read_csv(val_path)
    print(f"\nValidation ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {val_data.shape}")
    print(f"  - ì‚¬ìš©ì ìˆ˜: {val_data['user_idx'].nunique()}")
    print(f"  - ì•„ì´í…œ ìˆ˜: {val_data['item_idx'].nunique()}")
    print(f"  - ìƒí˜¸ì‘ìš© ìˆ˜: {len(val_data)}")
    
    # Grid Search íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
    param_names = list(param_grid.keys())
    param_values = list(param_grid.values())
    param_combinations = list(product(*param_values))
    
    total_combinations = len(param_combinations)
    print(f"\nì´ {total_combinations}ê°œì˜ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    print(f"í‰ê°€ ì§€í‘œ: Precision, Recall, NDCG, Hit Rate @ K={eval_k_list}")
    
    # ê²°ê³¼ ì €ì¥
    all_results = []
    best_score = -1
    best_params = None
    best_metrics = None
    
    # ê° íŒŒë¼ë¯¸í„° ì¡°í•©ì— ëŒ€í•´ í•™ìŠµ ë° í‰ê°€
    for idx, params in enumerate(param_combinations):
        param_dict = dict(zip(param_names, params))
        
        print("\n" + "=" * 80)
        print(f"ì‹¤í—˜ {idx + 1}/{total_combinations}")
        print("-" * 80)
        print("íŒŒë¼ë¯¸í„°:")
        for key, value in param_dict.items():
            print(f"  {key}: {value}")
        print("-" * 80)
        
        try:
            # ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            recommender = HybridRecommender(device=device)
            
            # ë°ì´í„° ë¡œë“œ
            print("\n1. ë°ì´í„° ë¡œë“œ ì¤‘...")
            recommender.load_data(data_path)
            
            # ëª¨ë¸ í•™ìŠµ
            print("\n2. ëª¨ë¸ í•™ìŠµ ì¤‘...")
            recommender.train_models(
                mf_factors=param_dict.get('mf_factors', 50),
                epochs=param_dict.get('epochs', 30),
                batch_size=param_dict.get('batch_size', 512)
            )
            
            # Validation set í‰ê°€
            print("\n3. Validation set í‰ê°€ ì¤‘...")
            metrics = RecommendationEvaluator.evaluate_recommendations(
                recommender=recommender,
                val_data=val_data,
                top_k=param_dict.get('top_k', 250),
                top_n=param_dict.get('top_n', 50),
                eval_k_list=eval_k_list
            )
            
            # ê²°ê³¼ ì¶œë ¥
            print("\ní‰ê°€ ê²°ê³¼:")
            for metric_name, value in metrics.items():
                if metric_name not in ['evaluated_users', 'total_users']:
                    print(f"  {metric_name}: {value:.4f}")
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (NDCG@10ì„ ì£¼ìš” ì§€í‘œë¡œ ì‚¬ìš©)
            composite_score = metrics.get('NDCG@10', 0.0)
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'experiment_id': idx + 1,
                'params': param_dict,
                'metrics': metrics,
                'composite_score': composite_score
            }
            all_results.append(result)
            
            # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
            if composite_score > best_score:
                best_score = composite_score
                best_params = param_dict
                best_metrics = metrics
                
                print(f"\nğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜! NDCG@10: {best_score:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                print(f"\nìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì¤‘...")
                best_model_path = os.path.join(model_save_path, "best_model")
                recommender.save_models(best_model_path)
                
                # ìµœê³  íŒŒë¼ë¯¸í„° ì €ì¥
                best_params_file = os.path.join(model_save_path, "best_params.json")
                with open(best_params_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'params': best_params,
                        'metrics': best_metrics,
                        'composite_score': best_score
                    }, f, indent=2, ensure_ascii=False)
            
        except Exception as e:
            print(f"\nâŒ ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("Grid Search ì™„ë£Œ!")
    print("=" * 80)
    
    if best_params:
        print("\nğŸ† ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for key, value in best_params.items():
            print(f"  {key}: {value}")
        
        print(f"\nğŸ“Š ìµœê³  ì„±ëŠ¥ ì§€í‘œ (NDCG@10: {best_score:.4f}):")
        for metric_name, value in best_metrics.items():
            if metric_name not in ['evaluated_users', 'total_users']:
                print(f"  {metric_name}: {value:.4f}")
    
    # ì „ì²´ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
    results_file = os.path.join(model_save_path, f"grid_search_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    
    results_df_data = []
    for result in all_results:
        row = result['params'].copy()
        row.update(result['metrics'])
        row['composite_score'] = result['composite_score']
        results_df_data.append(row)
    
    results_df = pd.DataFrame(results_df_data)
    results_df.to_csv(results_file, index=False, encoding='utf-8')
    print(f"\nì „ì²´ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_file}")
    
    return best_params, best_metrics, all_results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    data_path = os.path.join(project_root, "data", "splitty_recommendation_data_1")
    model_save_path = os.path.join(current_dir, "saved_models")
    
    os.makedirs(model_save_path, exist_ok=True)
    
    print(f"ë°ì´í„° ê²½ë¡œ: {data_path}")
    print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_save_path}")
    
    # Grid Search íŒŒë¼ë¯¸í„° ì •ì˜
    param_grid = {
        'mf_factors': [30, 50, 70],           # Matrix Factorization ì ì¬ ìš”ì¸ ìˆ˜
        'epochs': [20, 30, 40],                # Two-Tower í•™ìŠµ ì—í¬í¬
        'batch_size': [256, 512, 1024],       # ë°°ì¹˜ í¬ê¸°
        'top_k': [150, 250, 350],             # MF í›„ë³´ ê°œìˆ˜
        'top_n': [50],                         # ìµœì¢… ì¶”ì²œ ê°œìˆ˜ (ê³ ì •)
    }
    
    # í‰ê°€í•  K ê°’ë“¤
    eval_k_list = [5, 10, 20]
    
    # Grid Search ì‹¤í–‰
    best_params, best_metrics, all_results = grid_search_hyperparameters(
        data_path=data_path,
        model_save_path=model_save_path,
        param_grid=param_grid,
        eval_k_list=eval_k_list,
        device='cpu'
    )
    
    print("\n" + "=" * 80)
    print("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nìµœê³  ì„±ëŠ¥ ëª¨ë¸ì´ {model_save_path}/best_model ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ì´ì œ FastAPI ì„œë²„ì—ì„œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
