#!/usr/bin/env python3
"""
AWS EC2 ìµœì í™” ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

íŠ¹ì§•:
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
- GPU/CPU ìë™ ê°ì§€
- í•™ìŠµ ì§„í–‰ë¥  ìƒì„¸ ì¶œë ¥
- ëª¨ë¸ ì €ì¥ ë° ë°±ì—…
"""

import os
import sys
import time
import psutil
import torch
import pandas as pd
from hybrid_recommender import HybridRecommendationSystem

def get_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
    print("=== ì‹œìŠ¤í…œ ì •ë³´ ===")
    print(f"CPU ì½”ì–´ ìˆ˜: {psutil.cpu_count()}")
    print(f"ë©”ëª¨ë¦¬: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {psutil.virtual_memory().available / (1024**3):.1f} GB")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
        device = "cuda"
    else:
        print("GPU: ì—†ìŒ (CPU ì‚¬ìš©)")
        device = "cpu"
    
    return device

def optimize_batch_size(available_memory_gb, device):
    """ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •"""
    if device == "cuda":
        # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        if gpu_memory_gb >= 16:
            return 1024
        elif gpu_memory_gb >= 8:
            return 512
        else:
            return 256
    else:
        # CPU ë©”ëª¨ë¦¬ ê¸°ë°˜
        if available_memory_gb >= 16:
            return 1024
        elif available_memory_gb >= 8:
            return 512
        elif available_memory_gb >= 4:
            return 256
        else:
            return 128

def monitor_training_progress(start_time, epoch, total_epochs):
    """í•™ìŠµ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§"""
    elapsed = time.time() - start_time
    progress = (epoch + 1) / total_epochs
    eta = elapsed / progress - elapsed if progress > 0 else 0
    
    print(f"ì§„í–‰ë¥ : {progress*100:.1f}% | "
          f"ê²½ê³¼ ì‹œê°„: {elapsed/60:.1f}ë¶„ | "
          f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta/60:.1f}ë¶„ | "
          f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {psutil.virtual_memory().percent:.1f}%")

def main():
    print("=== AWS EC2 ìµœì í™” ëª¨ë¸ í•™ìŠµ ===")
    
    # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
    device = get_system_info()
    available_memory = psutil.virtual_memory().available / (1024**3)
    
    # ë°°ì¹˜ í¬ê¸° ìµœì í™”
    optimal_batch_size = optimize_batch_size(available_memory, device)
    print(f"\nìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
    
    # ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    data_path = os.path.join(project_root, "data", "splitty_recommendation_data_1")
    model_path = os.path.join(current_dir, "saved_models")
    backup_path = os.path.join(current_dir, "model_backup")
    
    # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(backup_path, exist_ok=True)
    
    print(f"\në°ì´í„° ê²½ë¡œ: {data_path}")
    print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_path}")
    print(f"ë°±ì—… ê²½ë¡œ: {backup_path}")
    
    # ë°ì´í„° ì¡´ì¬ í™•ì¸
    train_file = os.path.join(data_path, "user_item_train.csv")
    if not os.path.exists(train_file):
        print(f"\nâŒ ì˜¤ë¥˜: í•™ìŠµ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_file}")
        return
    
    # ë°ì´í„° í¬ê¸° í™•ì¸
    train_df = pd.read_csv(train_file)
    data_size_mb = train_df.memory_usage(deep=True).sum() / (1024**2)
    print(f"\në°ì´í„° í¬ê¸°: {len(train_df):,}í–‰, {data_size_mb:.1f}MB")
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„° ì¡°ì •
    n_users = train_df['user_id'].nunique()
    n_items = train_df['item_id'].nunique()
    
    # ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì—í¬í¬ ì¡°ì •
    if len(train_df) > 100000:
        mf_epochs = 80
        tt_epochs = 30
    elif len(train_df) > 50000:
        mf_epochs = 100
        tt_epochs = 40
    else:
        mf_epochs = 120
        tt_epochs = 50
    
    print(f"\ní•™ìŠµ ì„¤ì •:")
    print(f"- ì‚¬ìš©ì ìˆ˜: {n_users:,}")
    print(f"- ì•„ì´í…œ ìˆ˜: {n_items:,}")
    print(f"- MF ì—í¬í¬: {mf_epochs}")
    print(f"- Two-Tower ì—í¬í¬: {tt_epochs}")
    print(f"- ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
    print(f"- ë””ë°”ì´ìŠ¤: {device.upper()}")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print(f"\nğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    recommender = HybridRecommendationSystem(device=device)
    
    try:
        # í•™ìŠµ ì‹œì‘
        start_time = time.time()
        
        print("\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
        recommender.load_data(data_path)
        
        print("\nğŸ¤– ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        # Matrix Factorization íŒŒë¼ë¯¸í„° ì¡°ì •
        class CustomHybridRecommender(HybridRecommendationSystem):
            def train_models(self, mf_factors=50, epochs=50, batch_size=1024):
                """EC2 ìµœì í™”ëœ í•™ìŠµ ë©”ì„œë“œ"""
                print("=== EC2 ìµœì í™” í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í•™ìŠµ ===")
                
                # 1ë‹¨ê³„: Implicit Matrix Factorization
                print(f"\n1ë‹¨ê³„: Implicit Matrix Factorization í•™ìŠµ")
                from models import ImplicitMatrixFactorization
                
                self.matrix_factorization = ImplicitMatrixFactorization(
                    n_factors=mf_factors,
                    learning_rate=0.01,
                    regularization=0.01,
                    n_epochs=mf_epochs,  # EC2 ìµœì í™”
                    negative_samples=3   # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¤„ì„
                )
                
                mf_start = time.time()
                self.matrix_factorization.fit(self.train_data, use_negative_sampling=True)
                mf_time = time.time() - mf_start
                print(f"MF í•™ìŠµ ì™„ë£Œ ({mf_time/60:.1f}ë¶„)")
                
                # 2ë‹¨ê³„: Two-Tower ëª¨ë¸
                print(f"\n2ë‹¨ê³„: Two-Tower ëª¨ë¸ í•™ìŠµ")
                
                n_users = self.train_data['user_id'].nunique()
                n_items = self.train_data['item_id'].nunique()
                n_categories = self.train_data['category'].nunique()
                
                from models import TwoTowerModel, TwoTowerTrainer
                
                # ëª¨ë¸ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
                embedding_dim = min(64, max(32, n_users // 100))
                hidden_dims = [min(128, embedding_dim * 2), embedding_dim]
                
                two_tower_model = TwoTowerModel(
                    n_users=n_users,
                    n_items=n_items,
                    n_categories=n_categories,
                    embedding_dim=embedding_dim,
                    hidden_dims=hidden_dims
                )
                
                self.two_tower_trainer = TwoTowerTrainer(two_tower_model, device=device)
                
                tt_start = time.time()
                self.two_tower_trainer.fit(
                    train_df=self.train_data,
                    epochs=tt_epochs,  # EC2 ìµœì í™”
                    batch_size=optimal_batch_size,
                    lr=0.001
                )
                tt_time = time.time() - tt_start
                print(f"Two-Tower í•™ìŠµ ì™„ë£Œ ({tt_time/60:.1f}ë¶„)")
                
                self.is_trained = True
                total_time = time.time() - start_time
                print(f"\nâœ… ì „ì²´ í•™ìŠµ ì™„ë£Œ ({total_time/60:.1f}ë¶„)")
        
        # ì»¤ìŠ¤í…€ í•™ìŠµ ì‹¤í–‰
        custom_recommender = CustomHybridRecommender(device=device)
        custom_recommender.train_data = recommender.train_data
        custom_recommender.item_meta = recommender.item_meta
        
        custom_recommender.train_models(
            mf_factors=50,
            epochs=tt_epochs,
            batch_size=optimal_batch_size
        )
        
        # ëª¨ë¸ ì €ì¥
        print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        custom_recommender.save_models(model_path)
        
        # ë°±ì—… ì €ì¥
        import shutil
        if os.path.exists(model_path):
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            backup_dir = os.path.join(backup_path, f"model_{timestamp}")
            shutil.copytree(model_path, backup_dir)
            print(f"ë°±ì—… ì €ì¥: {backup_dir}")
        
        # ì‹œìŠ¤í…œ í†µê³„
        print("\nğŸ“ˆ ì‹œìŠ¤í…œ í†µê³„:")
        stats = custom_recommender.get_system_stats()
        for key, value in stats.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for k, v in value.items():
                    print(f"    {k}: {v}")
            else:
                print(f"  {key}: {value}")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª ê°„ë‹¨í•œ ì¶”ì²œ í…ŒìŠ¤íŠ¸:")
        sample_users = custom_recommender.train_data['user_id'].unique()[:3]
        
        for user_id in sample_users:
            try:
                recommendations = custom_recommender.get_recommendations(
                    user_id=user_id, top_k=50, top_n=5
                )
                print(f"  ì‚¬ìš©ì {user_id}: {len(recommendations)}ê°œ ì¶”ì²œ")
            except Exception as e:
                print(f"  ì‚¬ìš©ì {user_id}: ì¶”ì²œ ì‹¤íŒ¨ - {str(e)}")
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ EC2 í•™ìŠµ ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„)")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print(f"\nìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {psutil.virtual_memory().percent:.1f}%")

if __name__ == "__main__":
    main()