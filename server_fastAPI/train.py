#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ëª¨ë“œ:
1. simple: ë¹ ë¥¸ í•™ìŠµ (ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
2. tune: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Grid Search)

ì‚¬ìš©ë²•:
python train.py                    # ê°„ë‹¨ í•™ìŠµ
python train.py --mode tune        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
python train.py --mode tune --quick  # ë¹ ë¥¸ íŠœë‹ (ì‘ì€ Grid)
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
from itertools import product
import json
from datetime import datetime
from hybrid_recommender import HybridRecommender


class RecommendationEvaluator:
    """ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ í´ë˜ìŠ¤"""
    
    @staticmethod
    def precision_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """Precision@K ê³„ì‚°"""
        if k == 0:
            return 0.0
        
        recommended_k = recommended_items[:k]
        relevant_set = set(relevant_items)
        
        hits = sum(1 for item in recommended_k if item in relevant_set)
        return hits / k
    
    @staticmethod
    def recall_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """Recall@K ê³„ì‚°"""
        if len(relevant_items) == 0:
            return 0.0
        
        recommended_k = recommended_items[:k]
        relevant_set = set(relevant_items)
        
        hits = sum(1 for item in recommended_k if item in relevant_set)
        return hits / len(relevant_items)
    
    @staticmethod
    def dcg_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """DCG@K ê³„ì‚°"""
        recommended_k = recommended_items[:k]
        relevant_set = set(relevant_items)
        
        dcg = 0.0
        for i, item in enumerate(recommended_k):
            if item in relevant_set:
                dcg += 1.0 / np.log2(i + 2)
        
        return dcg
    
    @staticmethod
    def ndcg_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """NDCG@K ê³„ì‚°"""
        dcg = RecommendationEvaluator.dcg_at_k(recommended_items, relevant_items, k)
        
        ideal_relevant = relevant_items[:k]
        idcg = RecommendationEvaluator.dcg_at_k(ideal_relevant, relevant_items, k)
        
        if idcg == 0.0:
            return 0.0
        
        return dcg / idcg
    
    @staticmethod
    def hit_rate_at_k(recommended_items: List[int], relevant_items: List[int], k: int) -> float:
        """Hit Rate@K ê³„ì‚°"""
        recommended_k = recommended_items[:k]
        relevant_set = set(relevant_items)
        
        hits = sum(1 for item in recommended_k if item in relevant_set)
        return 1.0 if hits > 0 else 0.0
    
    @classmethod
    def evaluate_recommendations(cls, recommender: HybridRecommender, 
                                val_data: pd.DataFrame, 
                                top_k: int = 250,
                                top_n: int = 50,
                                eval_k_list: List[int] = [5, 10, 20]) -> Dict:
        """Validation setì— ëŒ€í•œ ì¶”ì²œ í‰ê°€"""
        
        print(f"\nValidation set í‰ê°€ ì‹œì‘ (top_k={top_k}, top_n={top_n})...")
        
        user_relevant_items = val_data.groupby('user_idx')['item_idx'].apply(list).to_dict()
        
        results = {k: {
            'precision': [],
            'recall': [],
            'ndcg': [],
            'hit_rate': []
        } for k in eval_k_list}
        
        total_users = len(user_relevant_items)
        evaluated_users = 0
        
        for i, (user_id, relevant_items) in enumerate(user_relevant_items.items()):
            if i % 10 == 0:
                print(f"  ì§„í–‰ë¥ : {i}/{total_users} ì‚¬ìš©ì í‰ê°€ ì™„ë£Œ...", end='\r')
            
            try:
                recommendations = recommender.get_recommendations(
                    user_id=str(user_id),
                    top_k=top_k,
                    top_n=top_n
                )
                
                if not recommendations:
                    continue
                
                recommended_items = [rec['item_id'] for rec in recommendations]
                
                for k in eval_k_list:
                    results[k]['precision'].append(
                        cls.precision_at_k(recommended_items, relevant_items, k)
                    )
                    results[k]['recall'].append(
                        cls.recall_at_k(recommended_items, relevant_items, k)
                    )
                    results[k]['ndcg'].append(
                        cls.ndcg_at_k(recommended_items, relevant_items, k)
                    )
                    results[k]['hit_rate'].append(
                        cls.hit_rate_at_k(recommended_items, relevant_items, k)
                    )
                
                evaluated_users += 1
                
            except Exception as e:
                continue
        
        print(f"\n  í‰ê°€ ì™„ë£Œ: {evaluated_users}/{total_users} ì‚¬ìš©ì")
        
        metrics = {}
        for k in eval_k_list:
            metrics[f'Precision@{k}'] = np.mean(results[k]['precision']) if results[k]['precision'] else 0.0
            metrics[f'Recall@{k}'] = np.mean(results[k]['recall']) if results[k]['recall'] else 0.0
            metrics[f'NDCG@{k}'] = np.mean(results[k]['ndcg']) if results[k]['ndcg'] else 0.0
            metrics[f'HitRate@{k}'] = np.mean(results[k]['hit_rate']) if results[k]['hit_rate'] else 0.0
        
        metrics['evaluated_users'] = evaluated_users
        metrics['total_users'] = total_users
        
        return metrics


def simple_train(data_path: str, model_save_path: str, device: str = 'cpu'):
    """ê°„ë‹¨ í•™ìŠµ ëª¨ë“œ"""
    
    print("=" * 80)
    print("ê°„ë‹¨ í•™ìŠµ ëª¨ë“œ")
    print("=" * 80)
    print(f"ë°ì´í„° ê²½ë¡œ: {data_path}")
    print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_save_path}")
    
    # ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    recommender = HybridRecommender(device=device)
    
    # ë°ì´í„° ë¡œë“œ
    print("\n1. ë°ì´í„° ë¡œë“œ ì¤‘...")
    recommender.load_data(data_path)
    
    # ëª¨ë¸ í•™ìŠµ (ê¸°ë³¸ íŒŒë¼ë¯¸í„°)
    print("\n2. ëª¨ë¸ í•™ìŠµ ì¤‘...")
    recommender.train_models(
        mf_factors=50,
        epochs=30,
        batch_size=512
    )
    
    # ëª¨ë¸ ì €ì¥
    print("\n3. ëª¨ë¸ ì €ì¥ ì¤‘...")
    recommender.save_models(model_save_path)
    
    print("\n" + "=" * 80)
    print("í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)
    print(f"âœ… ëª¨ë¸ì´ {model_save_path}/ ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def grid_search_train(data_path: str, model_save_path: str, 
                     param_grid: Dict, eval_k_list: List[int], 
                     device: str = 'cpu'):
    """Grid Search í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
    
    print("=" * 80)
    print("í•˜ì´í¼íŒŒë¼ë¯¸í„° Grid Search ì‹œì‘")
    print("=" * 80)
    
    # Validation ë°ì´í„° ë¡œë“œ
    val_path = os.path.join(data_path, "user_item_val.csv")
    if not os.path.exists(val_path):
        raise FileNotFoundError(f"Validation ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {val_path}")
    
    val_data = pd.read_csv(val_path)
    print(f"\nValidation ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {val_data.shape}")
    print(f"  - ì‚¬ìš©ì ìˆ˜: {val_data['user_idx'].nunique()}")
    print(f"  - ì•„ì´í…œ ìˆ˜: {val_data['item_idx'].nunique()}")
    print(f"  - ìƒí˜¸ì‘ìš© ìˆ˜: {len(val_data)}")
    
    # Grid Search íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
    param_names = list(param_grid.keys())
    param_values = list(param_grid.values())
    param_combinations = list(product(*param_values))
    
    total_combinations = len(param_combinations)
    print(f"\nì´ {total_combinations}ê°œì˜ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    print(f"í‰ê°€ ì§€í‘œ: Precision, Recall, NDCG, Hit Rate @ K={eval_k_list}")
    
    # ê²°ê³¼ ì €ì¥
    all_results = []
    best_score = -1
    best_params = None
    best_metrics = None
    
    # ê° íŒŒë¼ë¯¸í„° ì¡°í•©ì— ëŒ€í•´ í•™ìŠµ ë° í‰ê°€
    for idx, params in enumerate(param_combinations):
        param_dict = dict(zip(param_names, params))
        
        print("\n" + "=" * 80)
        print(f"ì‹¤í—˜ {idx + 1}/{total_combinations}")
        print("-" * 80)
        print("íŒŒë¼ë¯¸í„°:")
        for key, value in param_dict.items():
            print(f"  {key}: {value}")
        print("-" * 80)
        
        try:
            # ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            recommender = HybridRecommender(device=device)
            
            # ë°ì´í„° ë¡œë“œ
            print("\n1. ë°ì´í„° ë¡œë“œ ì¤‘...")
            recommender.load_data(data_path)
            
            # ëª¨ë¸ í•™ìŠµ
            print("\n2. ëª¨ë¸ í•™ìŠµ ì¤‘...")
            recommender.train_models(
                mf_factors=param_dict.get('mf_factors', 50),
                epochs=param_dict.get('epochs', 30),
                batch_size=param_dict.get('batch_size', 512)
            )
            
            # Validation set í‰ê°€
            print("\n3. Validation set í‰ê°€ ì¤‘...")
            metrics = RecommendationEvaluator.evaluate_recommendations(
                recommender=recommender,
                val_data=val_data,
                top_k=param_dict.get('top_k', 250),
                top_n=param_dict.get('top_n', 50),
                eval_k_list=eval_k_list
            )
            
            # ê²°ê³¼ ì¶œë ¥
            print("\ní‰ê°€ ê²°ê³¼:")
            for metric_name, value in metrics.items():
                if metric_name not in ['evaluated_users', 'total_users']:
                    print(f"  {metric_name}: {value:.4f}")
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (NDCG@10ì„ ì£¼ìš” ì§€í‘œë¡œ ì‚¬ìš©)
            composite_score = metrics.get('NDCG@10', 0.0)
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'experiment_id': idx + 1,
                'params': param_dict,
                'metrics': metrics,
                'composite_score': composite_score
            }
            all_results.append(result)
            
            # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
            if composite_score > best_score:
                best_score = composite_score
                best_params = param_dict
                best_metrics = metrics
                
                print(f"\nğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜! NDCG@10: {best_score:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (API ì„œë²„ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ë£¨íŠ¸ì— ì €ì¥)
                print(f"\nìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì¤‘...")
                recommender.save_models(model_save_path)
                
                # ìµœê³  íŒŒë¼ë¯¸í„° ì €ì¥ (ì°¸ê³ ìš©)
                best_params_file = os.path.join(model_save_path, "best_params.json")
                with open(best_params_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'params': best_params,
                        'metrics': best_metrics,
                        'composite_score': best_score,
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }, f, indent=2, ensure_ascii=False)
                print(f"  âœ“ ëª¨ë¸: {model_save_path}/matrix_factorization.pkl, two_tower_model.pth")
                print(f"  âœ“ íŒŒë¼ë¯¸í„°: {best_params_file}")
            
        except Exception as e:
            print(f"\nâŒ ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("Grid Search ì™„ë£Œ!")
    print("=" * 80)
    
    if best_params:
        print("\nğŸ† ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for key, value in best_params.items():
            print(f"  {key}: {value}")
        
        print(f"\nğŸ“Š ìµœê³  ì„±ëŠ¥ ì§€í‘œ (NDCG@10: {best_score:.4f}):")
        for metric_name, value in best_metrics.items():
            if metric_name not in ['evaluated_users', 'total_users']:
                print(f"  {metric_name}: {value:.4f}")
        
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_save_path}/")
        print(f"   - matrix_factorization.pkl")
        print(f"   - two_tower_model.pth")
        print(f"   - best_params.json")
        print(f"\nâœ… API ì„œë²„(main.py)ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!")
    
    # ì „ì²´ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
    results_file = os.path.join(model_save_path, f"grid_search_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    
    results_df_data = []
    for result in all_results:
        row = result['params'].copy()
        row.update(result['metrics'])
        row['composite_score'] = result['composite_score']
        results_df_data.append(row)
    
    results_df = pd.DataFrame(results_df_data)
    results_df.to_csv(results_file, index=False, encoding='utf-8')
    print(f"\nğŸ“„ ì „ì²´ ì‹¤í—˜ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_file}")
    
    return best_params, best_metrics, all_results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(description='í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì‹œìŠ¤í…œ í•™ìŠµ')
    parser.add_argument('--mode', type=str, default='simple', 
                       choices=['simple', 'tune'],
                       help='í•™ìŠµ ëª¨ë“œ: simple (ê°„ë‹¨ í•™ìŠµ) ë˜ëŠ” tune (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)')
    parser.add_argument('--quick', action='store_true',
                       help='ë¹ ë¥¸ íŠœë‹ ëª¨ë“œ (ì‘ì€ Grid ì‚¬ìš©)')
    parser.add_argument('--device', type=str, default='cpu',
                       choices=['cpu', 'cuda'],
                       help='í•™ìŠµ ë””ë°”ì´ìŠ¤')
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    data_path = os.path.join(project_root, "data", "splitty_recommendation_data_1")
    model_save_path = os.path.join(current_dir, "saved_models")
    
    os.makedirs(model_save_path, exist_ok=True)
    
    print(f"ë°ì´í„° ê²½ë¡œ: {data_path}")
    print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_save_path}")
    print(f"ë””ë°”ì´ìŠ¤: {args.device}\n")
    
    if args.mode == 'simple':
        # ê°„ë‹¨ í•™ìŠµ
        simple_train(data_path, model_save_path, args.device)
        
    else:  # tune
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
        if args.quick:
            # ë¹ ë¥¸ íŠœë‹ (ì‘ì€ Grid)
            print("âš ï¸  ë¹ ë¥¸ íŠœë‹ ëª¨ë“œ: ì‘ì€ Gridë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n")
            param_grid = {
                'mf_factors': [30, 50],
                'epochs': [20, 30],
                'batch_size': [512],
                'top_k': [200, 250],
                'top_n': [50],
            }
            eval_k_list = [5, 10]
            print(f"ì´ {2 * 2 * 1 * 2 * 1}ê°€ì§€ ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
            print("ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 20-30ë¶„\n")
        else:
            # ì „ì²´ íŠœë‹ (í° Grid)
            param_grid = {
                'mf_factors': [30, 50, 70],
                'epochs': [20, 30, 40],
                'batch_size': [256, 512, 1024],
                'top_k': [150, 250, 350],
                'top_n': [50],
            }
            eval_k_list = [5, 10, 20]
            print(f"ì´ {3 * 3 * 3 * 3 * 1}ê°€ì§€ ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
            print("ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 2-3ì‹œê°„\n")
        
        grid_search_train(
            data_path=data_path,
            model_save_path=model_save_path,
            param_grid=param_grid,
            eval_k_list=eval_k_list,
            device=args.device
        )
    
    print("\n" + "=" * 80)
    print("í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nâœ… ëª¨ë¸ì´ {model_save_path}/ ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("   API ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ë©´ ìë™ìœ¼ë¡œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
