#!/usr/bin/env python3
"""
Splitty ì¶”ì²œ ì‹œìŠ¤í…œ í•™ìŠµ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ìƒì„±ë˜ëŠ” íŒŒì¼ë“¤:
- user_item_train.csv: í•™ìŠµ ë°ì´í„°ì…‹
- user_item_val.csv: ê²€ì¦ ë°ì´í„°ì…‹
- user_item_test.csv: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
- encoders.json: ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë” ì •ë³´
"""

import os
import json
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.preprocessing import LabelEncoder, StandardScaler
from collections import defaultdict


# ë°±ì—”ë“œ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (int â†’ í•œê¸€ëª…)
CATEGORY_MAPPING = {
    1: "ì‹í’ˆ",
    2: "ìƒí™œ/ì£¼ë°©",
    3: "ë·°í‹°/ë¯¸ìš©",
    4: "íŒ¨ì…˜",
    5: "ê±´ê°•/ìš´ë™",
    6: "ìœ ì•„"
}


def make_synthetic_data(n_users=200, n_items=500, n_events=5000, seed=42):
    """
    ì¶”ì²œ ì‹œìŠ¤í…œ í•™ìŠµì„ ìœ„í•œ ê°€ìƒ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        n_users: ìƒì„±í•  ì‚¬ìš©ì ìˆ˜
        n_items: ìƒì„±í•  ì•„ì´í…œ ìˆ˜
        n_events: ìƒì„±í•  ì´ë²¤íŠ¸ ìˆ˜
        seed: ëœë¤ ì‹œë“œ
    
    Returns:
        df: ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
        item_meta: ì•„ì´í…œ ë©”íƒ€ë°ì´í„°
        user_ids: ì‚¬ìš©ì ID ë¦¬ìŠ¤íŠ¸
        item_ids: ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
    """
    np.random.seed(seed)
    user_ids = [f"user_{i}" for i in range(1, n_users + 1)]
    item_ids = [f"item_{i}" for i in range(1, n_items + 1)]

    # === ì¹´í…Œê³ ë¦¬ë³„ ë¸Œëœë“œ ë° ì œí’ˆ ëª©ë¡ ===
    category_data = {
        1: {  # ì‹í’ˆ
            "brands": ["ì‚¼ë‹¤ìˆ˜", "ì•„ì´ì‹œìŠ¤", "í©ì‹œ", "ì½”ì¹´ì½œë¼", "ì¹ ì„±"],
            "products": ["ìƒìˆ˜", "ì½œë¼", "ì‚¬ì´ë‹¤", "ì œë¡œì½œë¼", "ìƒìˆ˜ ë²ˆë“¤", "ë¬¶ìŒ ìº”", "ìº”", "ë³‘", "í°ë³‘"]
        },
        2: {  # ìƒí™œ/ì£¼ë°©
            "brands": ["ë¹„íŠ¸", "í…Œí¬", "í¼ì‹¤", "ë‹¤ìš°ë‹ˆ"],
            "products": ["ì„¬ìœ ìœ ì—°ì œ", "ì„¸ì œ", "ìº¡ìŠì„¸ì œ", "ì‹œíŠ¸í˜•", "ì‹œíŠ¸í˜• ì„¸ì œ", "ìº¡ìŠí˜•"]
        },
        3: {  # ë·°í‹°/ë¯¸ìš©
            "brands": ["í”¼ì§€ì˜¤ê²”", "ë‹¥í„°ì§€", "ì˜¤íœ˜", "ë„¤ì´ì²˜ë¦¬í¼ë¸”ë¦­", "ê³¼ì¼ë‚˜ë¼"],
            "products": ["ìˆ˜ë¶„í¬ë¦¼", "ì•°í”Œ", "ë¡œì…˜", "ìì™¸ì„ ì°¨ë‹¨ì œ", "ì„ í¬ë¦¼", "ì¬í¬ë¦¼", "ì•Œë¡œì—"]
        },
        4: {  # íŒ¨ì…˜
            "brands": ["ìë¼", "ìœ ë‹ˆí´ë¡œ", "H&M", "ë¬´ì‹ ì‚¬", "ì—ì´ë¸”ë¦¬"],
            "products": ["í‹°ì…”ì¸ ", "ë°”ì§€", "ì›í”¼ìŠ¤", "ìì¼“", "í›„ë“œ", "ë§¨íˆ¬ë§¨", "ì…”ì¸ ", "ë‹ˆíŠ¸", "ì½”íŠ¸"]
        },
        5: {  # ê±´ê°•/ìš´ë™
            "brands": ["ë§›ìˆë‹­", "ë­ì»¤", "ì‡ë©”ì´íŠ¸", "ë­í‚¹ë‹­ì»´", "ì…€ë ‰ìŠ¤", "ì¹¼ë¡œë°”ì´", "ë§ˆì´í”„ë¡œí‹´"],
            "products": ["ë‹­ê°€ìŠ´ì‚´ íŒ©", "ë‹­ê°€ìŠ´ì‚´ íë¸Œ", "í›„ì¶”ë§› ë‹­ê°€ìŠ´ì‚´", "ë‹­ê°€ìŠ´ì‚´ ê°ˆë¦­", "ì´ˆì½”ë§› íŒŒìš°ë”", "í”„ë¡œí‹´ íŒŒìš°ë”", "í”„ë¡œí‹´", "ë¶€ìŠ¤í„°"]
        },
        6: {  # ìœ ì•„
            "brands": ["í—¤ê²", "íŒì•¤ê³ ", "íƒ€ì´ë‹ˆ íŠ¸ìœ™í´", "ë² ì´ë¹„ë©œ"],
            "products": ["ì –ë³‘", "ìª½ìª½ì´", "í„±ë°›ì´", "ëŒ€ìš©ëŸ‰ ì –ë³‘", "ê¸°ì €ê·€", "ë¬¼í‹°ìŠˆ", "ë¡œì…˜"]
        }
    }
    extras = ["10ê°œ", "20ê°œ", "30ê°œ", "5ê°œ", "24ê°œ"]
    # ======================================

    # 1. ì•„ì´í…œë³„ ì¹´í…Œê³ ë¦¬ ìš°ì„  í• ë‹¹ (ë°±ì—”ë“œ ê·œì¹™: 1~6)
    item_category_ids = np.random.choice(list(category_data.keys()), size=n_items)

    # 2. í• ë‹¹ëœ ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ì œëª© ìƒì„±
    item_titles = []
    for category_id in item_category_ids:
        brand = np.random.choice(category_data[category_id]["brands"])
        product = np.random.choice(category_data[category_id]["products"])
        extra = np.random.choice(extras)
        title = f"{brand} {product} {extra}"
        item_titles.append(title)

    # 3. item_meta ë°ì´í„°í”„ë ˆì„ ìƒì„±
    item_meta = pd.DataFrame({
        "item_id": item_ids,
        "category_idx": item_category_ids,  # ë°±ì—”ë“œì—ì„œ ì œê³µí•˜ëŠ” int ê°’ (1~6)
        "price": np.random.randint(500, 250000, size=n_items),
        "title": item_titles
    })

    # 4. ì´ë²¤íŠ¸ ë°ì´í„° ìƒì„±
    start_time = pd.Timestamp("2025-10-01")
    timestamps = [start_time + pd.Timedelta(seconds=int(x)) for x in np.random.randint(0, 86400 * 30, size=n_events)]
    df = pd.DataFrame({
        "user_id": np.random.choice(user_ids, size=n_events),
        "item_id": np.random.choice(item_ids, size=n_events),
        "action": np.random.choice(["view", "like", "enter", "purchase"], p=[0.7, 0.15, 0.09, 0.06], size=n_events),
        # view: ê²Œì‹œë¬¼ ë³´ê¸°, like: ì¢‹ì•„ìš”, enter: êµ¬ë§¤ ì§„ì…, purchase: ì‹¤ì œ êµ¬ë§¤
        "timestamp": timestamps
    })
    df = df.merge(item_meta[["item_id", "category_idx", "price", "title"]], on="item_id", how="left")
    
    return df, item_meta, user_ids, item_ids


def process_and_save(df, item_meta, user_ids, item_ids, output_dir="output"):
    """
    ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  í•™ìŠµì— í•„ìš”í•œ í˜•íƒœë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    - ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ì œê±° (ëª¨ë¸ í•™ìŠµ ì‹œ ìˆ˜í–‰)
    - Train/Val/Test ìŠ¤í‚¤ë§ˆ í†µì¼
    
    Args:
        df: ì´ë²¤íŠ¸ ë°ì´í„°í”„ë ˆì„
        item_meta: ì•„ì´í…œ ë©”íƒ€ë°ì´í„°
        user_ids: ì‚¬ìš©ì ID ë¦¬ìŠ¤íŠ¸
        item_ids: ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    os.makedirs(output_dir, exist_ok=True)

    # 1) ì•”ì‹œì  í”¼ë“œë°±ì„ ìœ„í•œ ë ˆì´ë¸”ê³¼ ê°€ì¤‘ì¹˜ ë§¤í•‘
    positive_actions = {"purchase": 1, "enter": 1, "like": 1, "view": 1}
    df["label"] = df["action"].map(positive_actions).fillna(0).astype(int)
    df["weight"] = df["action"].map({"purchase": 5, "enter": 3, "like": 2, "view": 1}).fillna(1)

    # 2) ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (user, itemë§Œ - category_idxëŠ” ì´ë¯¸ ë°±ì—”ë“œ ê·œì¹™ì— ë”°ë¼ 1~6)
    le_user = LabelEncoder().fit(df["user_id"].unique())
    le_item = LabelEncoder().fit(df["item_id"].unique())
    df["user_idx"] = le_user.transform(df["user_id"])
    df["item_idx"] = le_item.transform(df["item_id"])
    
    # category_idxëŠ” ì´ë¯¸ ë°±ì—”ë“œì—ì„œ ì œê³µí•œ ê°’ (1~6)ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    # df["category_idx"]ëŠ” ì´ë¯¸ ì¡´ì¬í•¨

    # 3) ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì •ê·œí™”
    scaler_price = StandardScaler().fit(item_meta[["price"]])
    df["price_norm"] = scaler_price.transform(df[["price"]])

    # 4) ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ë¶„í• 
    df = df.sort_values("timestamp").reset_index(drop=True)
    train_cut = df["timestamp"].quantile(0.70)
    val_cut = df["timestamp"].quantile(0.85)
    train_df = df[df["timestamp"] <= train_cut].copy()
    val_df = df[(df["timestamp"] > train_cut) & (df["timestamp"] <= val_cut)].copy()
    test_df = df[df["timestamp"] > val_cut].copy()

    # ì½œë“œìŠ¤íƒ€íŠ¸ ë¬¸ì œ ë°©ì§€: ìƒí˜¸ì‘ìš©ì´ ì ì€ ì‚¬ìš©ì ì œì™¸
    min_interactions = 3
    user_counts = train_df["user_id"].value_counts()
    active_users = user_counts[user_counts >= min_interactions].index
    train_df = train_df[train_df["user_id"].isin(active_users)].copy()

    # 5) í•™ìŠµì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ì œê±°)
    # ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì€ ImplicitMatrixFactorization í•™ìŠµ ì‹œ ìë™ìœ¼ë¡œ ìˆ˜í–‰ë¨
    required_cols = ["user_idx", "item_idx", "label", "weight", "price_norm", "category_idx"]
    
    train_final = train_df[required_cols].copy()
    val_final = val_df[required_cols].copy()
    test_final = test_df[required_cols].copy()

    # 6) ê²°ê³¼ ì €ì¥ (ëª¨ë“  ë°ì´í„°ì…‹ì´ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆ)
    train_final.to_csv(os.path.join(output_dir, "user_item_train.csv"), index=False)
    val_final.to_csv(os.path.join(output_dir, "user_item_val.csv"), index=False)
    test_final.to_csv(os.path.join(output_dir, "user_item_test.csv"), index=False)

    # 7) ì¸ì½”ë” ë©”íƒ€ë°ì´í„° ì €ì¥
    enc_meta = {
        "user_classes": le_user.classes_.tolist(), 
        "item_classes": le_item.classes_.tolist(),
        "category_mapping": CATEGORY_MAPPING,  # ë°±ì—”ë“œ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì •ë³´
        "price_mean": float(scaler_price.mean_[0]),
        "price_var": float(scaler_price.var_[0])
    }
    with open(os.path.join(output_dir, "encoders.json"), "w", encoding='utf-8') as f:
        json.dump(enc_meta, f, indent=2, ensure_ascii=False)

    # 8) í†µê³„ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ë°ì´í„° ìƒì„± ì™„ë£Œ")
    print("=" * 70)
    print(f"ì €ì¥ ê²½ë¡œ: {output_dir}")
    print(f"\nğŸ“Š ë°ì´í„° í¬ê¸°:")
    print(f"  - Train: {len(train_final):,}ê°œ (Positive only)")
    print(f"  - Val: {len(val_final):,}ê°œ")
    print(f"  - Test: {len(test_final):,}ê°œ")
    print(f"\nğŸ“‹ ìŠ¤í‚¤ë§ˆ: {list(train_final.columns)}")
    print(f"\nğŸ·ï¸  ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë°±ì—”ë“œ ê·œì¹™):")
    for cat_id, cat_name in CATEGORY_MAPPING.items():
        cat_count = train_final[train_final['category_idx'] == cat_id].shape[0]
        print(f"  - {cat_id}: {cat_name} ({cat_count:,}ê°œ)")
    print(f"\nâš ï¸  ì£¼ì˜: ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì€ ImplicitMatrixFactorization.fit() ì‹œ ìë™ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
    print("=" * 70)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 70)
    print("Splitty ì¶”ì²œ ì‹œìŠ¤í…œ ë°ì´í„° ìƒì„± ì‹œì‘")
    print("=" * 70)
    
    # ë°ì´í„° ìƒì„±
    print("\n[1/2] ê°€ìƒ ë°ì´í„° ìƒì„± ì¤‘...")
    df, item_meta, user_ids, item_ids = make_synthetic_data(
        n_users=200,
        n_items=500,
        n_events=7000,
        seed=42
    )
    
    # ì „ì²˜ë¦¬ ë° ì €ì¥
    print("\n[2/2] ë°ì´í„° ì „ì²˜ë¦¬ ë° ì €ì¥ ì¤‘...")
    current_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(os.path.dirname(current_dir), "splitty_recommendation_data_1")
    process_and_save(df, item_meta, user_ids, item_ids, output_dir=output_dir)
    
    # ê²€ì¦
    print("\nâœ… ìƒì„±ëœ ë°ì´í„° ìƒ˜í”Œ:")
    train = pd.read_csv(os.path.join(output_dir, "user_item_train.csv"))
    print(train.head(3))


if __name__ == "__main__":
    main()
